{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Library Importing</h2>\n",
    "<p>Import packages/libraries for data processing.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Anaconda\\envs\\py35\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Classification Pipeline</h2>\n",
    "<p>Here is the pipeline for processing raw data to generat insights/semantic meaning.</p>\n",
    "<ol>\n",
    "    <h4><li>Word2Vec - Distributed Representations of Words and Phrases and their Compositionality</li></h4>\n",
    "    <h4><li>PCA - Principal component analysis</li></h4>\n",
    "    <h4><li>KMean Clustering</li></h4>\n",
    "</ol>\n",
    "<p>Goal: Classify the semantic of topics by encoding topics to vectors.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GC_initialization():\n",
    "\tcurrent_directory = os.getcwd() + '\\\\data'\n",
    "\tclassification_filename = current_directory + '\\\\classification_dictionary.pickle'\n",
    "\twith open(classification_filename, 'rb') as f:\n",
    "\t\tglobal classification \n",
    "\t\tclassification = pickle.load(f)\n",
    "\tglobal model\n",
    "\tmodel = KeyedVectors.load_word2vec_format(current_directory + '\\\\GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\twith open(current_directory + '\\\\trained_pca.pickle', \"rb\") as f:\n",
    "\t\tglobal trained_pca\n",
    "\t\ttrained_pca = pickle.load(f)\n",
    "\twith open(current_directory + '\\\\trained_kmean.pickle', \"rb\") as f:\n",
    "\t\tglobal trained_kmean\n",
    "\t\ttrained_kmean = pickle.load(f)\n",
    "\n",
    "GC_initialization()\n",
    "\n",
    "def topic_classification(topic):\n",
    "\tprint('topic_classification')\n",
    "\tif topic not in classification:\n",
    "\t\tgroup = classify_new_topic(topic)\n",
    "\t\tprint(\"{0} in group {1}\".format(topic, group))\n",
    "\telse:\n",
    "\t\tgroup = classification[topic]\n",
    "\t\tprint(\"{0} in group {1}\".format(topic, group))\n",
    "\n",
    "\treturn group\n",
    "\n",
    "\n",
    "def classify_new_topic(topic):\n",
    "\tprint('classify_new_topic')\n",
    "\ttopic_vector_temp = model[topic]\n",
    "\ttopic_vector = np.empty([1,300])\n",
    "\ttopic_vector[0,]=topic_vector_temp\n",
    "\ttopic_vector = trained_pca.transform(topic_vector)\n",
    "\tgroup = trained_kmean.predict(topic_vector)[0]\n",
    "\tprint(\"{0} in group {1}\".format(topic, group))\n",
    "\treturn group\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Scoring and Best Matching Words</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BE_initialization():\n",
    "    current_directory = os.getcwd() + '\\\\data'\n",
    "    global list_of_good_words\n",
    "    list_of_good_words = pickle.load(open(current_directory + '\\\\list_of_good_words.pickle', \"rb\"))\n",
    "\n",
    "    # word_scores is a list of dict\n",
    "    # word_scores[i]['wwww'] is the score of the word 'wwww' in group i\n",
    "    global word_scores\n",
    "    word_scores = pickle.load(open(current_directory+\"\\\\word_scores.pickle\", \"rb\" ))\n",
    "\n",
    "    # max_scores is a list\n",
    "    # max_scores[i] is the max score of words stored in group i\n",
    "    global max_scores\n",
    "    max_scores = pickle.load(open(current_directory+\"\\\\max_scores.pickle\", \"rb\"))\n",
    "    global ignore_list\n",
    "    ignore_list = ['hong', 'kong']\n",
    "\n",
    "BE_initialization()\n",
    "    \n",
    "def stem_and_stop(raw):\n",
    "    tokens = word_tokenize(raw)\n",
    "\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    stem_stop_words = []\n",
    "\n",
    "    for t in tokens:\n",
    "        lemmatized = wnl.lemmatize(t.lower())\n",
    "        if (lemmatized not in stopWords) and (len(lemmatized) > 3) and (lemmatized not in ignore_list):\n",
    "            stem_stop_words.append(lemmatized)\n",
    "\n",
    "    return(stem_stop_words)\n",
    "\n",
    "\n",
    "def good_words(topic):\n",
    "    # TODO: call Henry function to determine group\n",
    "    try:\n",
    "        group = topic_classification(topic)\n",
    "    except KeyError:\n",
    "        print(\"No Such Key\")\n",
    "        return [\"ERROR: Invalid topic - Please check your spelling\"]\n",
    "    return list_of_good_words[group]\n",
    "\n",
    "def score(title, topic):\n",
    "    # TODO: call Henry function to determine Group\n",
    "    try:\n",
    "        group = topic_classification(topic)\n",
    "    except KeyError:\n",
    "        print(\"No Such Key\")\n",
    "        return -1\n",
    "    count = 0\n",
    "    total_score = 0\n",
    "    for word in stem_and_stop(title):\n",
    "        if word in word_scores[group]:\n",
    "            count += 1\n",
    "            total_score += word_scores[group][word]\n",
    "    if(count > 0):\n",
    "        return total_score / count / max_scores[group] * 10\n",
    "    else:\n",
    "        return 0 # all words in title do not in training data for that group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Consumer the Trained Model</h2>\n",
    "<h3>Module 1</h3>\n",
    "<h3>Get the word suggestions for a given topic</h3>\n",
    "<h4>Input: \"topic\"</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic = \"Tencent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(good_word(topic))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
